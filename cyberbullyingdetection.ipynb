{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Sai\n",
      "[nltk_data]     keerthan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Sai\n",
      "[nltk_data]     keerthan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m18153/18153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6720s\u001b[0m 370ms/step - accuracy: 0.8006 - loss: 0.4240 - val_accuracy: 0.8404 - val_loss: 0.3492\n",
      "Epoch 2/5\n",
      "\u001b[1m18153/18153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5072s\u001b[0m 279ms/step - accuracy: 0.8562 - loss: 0.3229 - val_accuracy: 0.8492 - val_loss: 0.3390\n",
      "Epoch 3/5\n",
      "\u001b[1m18153/18153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5745s\u001b[0m 317ms/step - accuracy: 0.8789 - loss: 0.2767 - val_accuracy: 0.8526 - val_loss: 0.3345\n",
      "Epoch 4/5\n",
      "\u001b[1m18153/18153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5111s\u001b[0m 282ms/step - accuracy: 0.8983 - loss: 0.2375 - val_accuracy: 0.8551 - val_loss: 0.3336\n",
      "Epoch 5/5\n",
      "\u001b[1m18153/18153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4354s\u001b[0m 240ms/step - accuracy: 0.9125 - loss: 0.2082 - val_accuracy: 0.8534 - val_loss: 0.3584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4539/4539\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m419s\u001b[0m 92ms/step\n",
      "Accuracy: 0.8534195449787914\n",
      "Precision: 0.8475913621262459\n",
      "Recall: 0.864582336945382\n",
      "F1 Score: 0.8560025434792903\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GRU, Dense, Dropout, Bidirectional, Attention, GlobalAveragePooling1D\n",
    "\n",
    "# Download NLTK stopwords and lemmatizer resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer and stop words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load CSV file\n",
    "csv_file = 'HateSpeechDatasetBalanced.csv'  # Replace with the path to your file\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Preprocess text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply text preprocessing\n",
    "df['cleaned_text'] = df['Content'].apply(clean_text)\n",
    "\n",
    "# Split the dataset\n",
    "X = df['cleaned_text'].values\n",
    "y = df['Label'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize text\n",
    "max_vocab_size = 10000\n",
    "max_sequence_length = 150\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_sequence_length, padding='post')\n",
    "\n",
    "# Define CNN-GRU model\n",
    "def build_cnn_gru_model(input_length, vocab_size):\n",
    "    inputs = tf.keras.Input(shape=(input_length,))\n",
    "    \n",
    "    # Embedding Layer\n",
    "    x = Embedding(vocab_size, 128)(inputs)\n",
    "    \n",
    "    # CNN Layer\n",
    "    x = Conv1D(128, 5, activation='relu')(x)\n",
    "    \n",
    "    # GRU Layer\n",
    "    x = Bidirectional(GRU(128, return_sequences=True))(x)\n",
    "    \n",
    "    # Attention Mechanism\n",
    "    attention = Attention()([x, x])\n",
    "    \n",
    "    # Global Pooling Layer\n",
    "    x = GlobalAveragePooling1D()(attention)\n",
    "    \n",
    "    # Fully Connected Layer\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    # Output Layer\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Build and compile model\n",
    "input_length = max_sequence_length\n",
    "model = build_cnn_gru_model(input_length, max_vocab_size)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_padded, y_train, epochs=5, batch_size=32, validation_data=(test_padded, y_test))\n",
    "\n",
    "# Save the model\n",
    "model.save('cyberbullying_cnn_gru_model.h5')\n",
    "\n",
    "# Predict on test data\n",
    "test_predictions = model.predict(test_padded)\n",
    "test_predictions = np.round(test_predictions).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, test_predictions)\n",
    "precision = precision_score(y_test, test_predictions)\n",
    "recall = recall_score(y_test, test_predictions)\n",
    "f1 = f1_score(y_test, test_predictions)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
